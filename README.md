# HÆ°á»›ng dáº«n Táº¡o Dá»¯ Liá»‡u Tá»•ng há»£p vá»›i GPT-OSS-120B

Dá»± Ã¡n nÃ y cung cáº¥p cÃ´ng cá»¥ Ä‘á»ƒ táº¡o dá»¯ liá»‡u tá»•ng há»£p quy mÃ´ lá»›n sá»­ dá»¥ng mÃ´ hÃ¬nh GPT-OSS-120B thÃ´ng qua API.

## ğŸš€ TÃ­nh nÄƒng

- Táº¡o dá»¯ liá»‡u tá»•ng há»£p tá»« prompts cÃ³ sáºµn
- Há»— trá»£ káº¿t ná»‘i API GPT-OSS-120B
- Xá»­ lÃ½ song song vá»›i nhiá»u instances
- Theo dÃµi tiáº¿n trÃ¬nh vá»›i WandB
- LÆ°u checkpoint Ä‘á»‹nh ká»³ Ä‘á»ƒ trÃ¡nh máº¥t dá»¯ liá»‡u
- Tá»± Ä‘á»™ng upload lÃªn Hugging Face Hub

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
synthetic_data_generation/
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ gpt_oss_config.yaml          # File cáº¥u hÃ¬nh API
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_generation.sh            # Script cháº¡y generation
â”‚   â””â”€â”€ slurm/
â”‚       â””â”€â”€ submit_slurm_job.sh      # Script cho HPC Slurm
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ token_counter.py             # Tiá»‡n Ã­ch Ä‘áº¿m token
â”‚   â””â”€â”€ data_processor.py            # Xá»­ lÃ½ dá»¯ liá»‡u
â”‚
â”œâ”€â”€ generate_synthetic_gpt_oss.py    # Script chÃ­nh
â”œâ”€â”€ requirements.txt                 # dependencies
â””â”€â”€ README.md                        # HÆ°á»›ng dáº«n nÃ y
```

## âš™ï¸ CÃ i Ä‘áº·t

### 1. Clone vÃ  thiáº¿t láº­p mÃ´i trÆ°á»ng

```bash
# Táº¡o thÆ° má»¥c dá»± Ã¡n
mkdir synthetic_data_generation
cd synthetic_data_generation

# Táº¡o virtual environment
python -m venv venv
source venv/bin/activate  # TrÃªn Windows: venv\Scripts\activate

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt
```

### 2. Thiáº¿t láº­p biáº¿n mÃ´i trÆ°á»ng

Táº¡o file `.env` hoáº·c export cÃ¡c biáº¿n mÃ´i trÆ°á»ng:

```bash
# API Keys
export API_KEY="your-gpt-oss-api-key-here"
export HF_TOKEN="your-huggingface-token-here"
export WANDB_API_KEY="your-wandb-api-key-here"

# Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n
export CHECKPOINT_PATH="./synthetic_data"
export CONFIG_FILE="config/gpt_oss_config.yaml"
```

### 3. Cáº¥u hÃ¬nh API endpoint

Chá»‰nh sá»­a file `config/gpt_oss_config.yaml`:

```yaml
model: "gpt-oss-120b"
api_base: "https://your-actual-gpt-oss-endpoint.com/v1"  # Thay báº±ng endpoint thá»±c táº¿
api_key: "${API_KEY}"  # Sá»­ dá»¥ng biáº¿n mÃ´i trÆ°á»ng
instances: 2
per_instance_max_parallel_requests: 8
request_timeout: 120
max_retries: 6
retry_delay: 4
```

## ğŸƒâ€â™‚ï¸ Cháº¡y Generation

### CÃ¡ch 1: Sá»­ dá»¥ng script (khuyáº¿n nghá»‹)

```bash
bash scripts/run_generation.sh
```

### CÃ¡ch 2: Cháº¡y trá»±c tiáº¿p vá»›i Python

```bash
python generate_synthetic_gpt_oss.py \
    --prompts_dataset "HuggingFaceTB/cosmopedia-100k" \
    --prompt_column "prompt" \
    --max_samples 5000 \
    --checkpoint_interval 500 \
    --max_new_tokens 2048 \
    --temperature 0.7 \
    --top_p 0.9 \
    --repetition_penalty 1.1 \
    --repo_id "your-username/synthetic_data_gpt_oss" \
    --wandb_username "your-wandb-username" \
    --config_file "config/gpt_oss_config.yaml"
```

### CÃ¡ch 3: Cháº¡y trÃªn Slurm cluster

```bash
bash scripts/slurm/submit_slurm_job.sh
```

## ğŸ“Š CÃ¡c tham sá»‘ quan trá»ng

| Tham sá»‘ | MÃ´ táº£ | GiÃ¡ trá»‹ máº·c Ä‘á»‹nh |
|---------|-------|------------------|
| `--max_samples` | Sá»‘ lÆ°á»£ng máº«u cáº§n generate (-1 Ä‘á»ƒ generate toÃ n bá»™) | 1000 |
| `--max_new_tokens` | Sá»‘ token tá»‘i Ä‘a cho má»—i generation | 2048 |
| `--temperature` | Äá»™ sÃ¡ng táº¡o cá»§a model | 0.7 |
| `--top_p` | Top-p sampling | 0.9 |
| `--checkpoint_interval` | LÆ°u checkpoint sau má»—i N máº«u | 100 |
| `--instances` | Sá»‘ instances cháº¡y song song | 2 |
| `--per_instance_max_parallel_requests` | Sá»‘ request song song má»—i instance | 8 |

## ğŸ‘€ Theo dÃµi tiáº¿n trÃ¬nh

### WandB Dashboard
Truy cáº­p [wandb.ai](https://wandb.ai) Ä‘á»ƒ theo dÃµi:
- Tá»‘c Ä‘á»™ generation (tokens/giÃ¢y)
- Tá»· lá»‡ thÃ nh cÃ´ng/tháº¥t báº¡i
- Cháº¥t lÆ°á»£ng generation
- Sá»­ dá»¥ng tÃ i nguyÃªn

### Checkpoint files
Dá»¯ liá»‡u Ä‘Æ°á»£c lÆ°u tá»± Ä‘á»™ng táº¡i: `./synthetic_data/your-dataset-name/data/`

## ğŸ“¦ Output

Dá»¯ liá»‡u Ä‘Æ°á»£c generate sáº½ bao gá»“m:
- `prompt`: Prompt gá»‘c
- `completion`: VÄƒn báº£n Ä‘Æ°á»£c generate
- `token_length`: Äá»™ dÃ i token
- `model`: TÃªn model Ä‘Æ°á»£c sá»­ dá»¥ng

Sau khi hoÃ n thÃ nh, dá»¯ liá»‡u sáº½ Ä‘Æ°á»£c tá»± Ä‘á»™ng upload lÃªn Hugging Face Hub.

## ğŸ› ï¸ TÃ¹y chá»‰nh

### ThÃªm Ä‘á»‹nh dáº¡ng dá»¯ liá»‡u má»›i

Chá»‰nh sá»­a `utils/data_processor.py` Ä‘á»ƒ thÃªm Ä‘á»‹nh dáº¡ng output má»›i:

```python
def format_textbook(sample):
    """Äá»‹nh dáº¡ng cho textbook"""
    return f"# {sample['title']}\n\n{sample['content']}"

def format_qa(sample):
    """Äá»‹nh dáº¡ng cho Q&A"""
    return f"Q: {sample['question']}\nA: {sample['answer']}"
```

### Äiá»u chá»‰nh prompt template

Sá»­a Ä‘á»•i hÃ m `process_text` trong script chÃ­nh Ä‘á»ƒ thay Ä‘á»•i template prompt.

## â“ Xá»­ lÃ½ sá»± cá»‘

### Lá»—i káº¿t ná»‘i API
- Kiá»ƒm tra API key vÃ  endpoint
- Äáº£m báº£o network cÃ³ thá»ƒ truy cáº­p endpoint
- Kiá»ƒm tra rate limits cá»§a API

### Lá»—i memory
- Giáº£m `--per_instance_max_parallel_requests`
- Giáº£m `--max_new_tokens`
- TÄƒng `--checkpoint_interval`

### Lá»—i timeout
- TÄƒng `--request_timeout` trong config
- Kiá»ƒm tra network stability

## ğŸ“ˆ Best Practices

1. **Báº¯t Ä‘áº§u nhá»**: Cháº¡y thá»­ vá»›i `--max_samples 100` trÆ°á»›c
2. **Theo dÃµi cost**: GiÃ¡m sÃ¡t token usage Ä‘á»ƒ kiá»ƒm soÃ¡t chi phÃ­
3. **Backup thÆ°á»ng xuyÃªn**: Sá»­ dá»¥ng checkpoint Ä‘á»ƒ trÃ¡nh máº¥t dá»¯ liá»‡u
4. **Validate cháº¥t lÆ°á»£ng**: Kiá»ƒm tra ngáº«u nhiÃªn cÃ¡c samples Ä‘Æ°á»£c generate

## ğŸ“ License

Dá»± Ã¡n nÃ y sá»­ dá»¥ng MIT License.

## ğŸ¤ ÄÃ³ng gÃ³p

ÄÃ³ng gÃ³p báº±ng cÃ¡ch:
1. Report bugs qua Issues
2. Gá»­i Pull Requests
3. Cáº£i thiá»‡n documentation
4. Chia sáº» use cases vÃ  feedback



*LÆ°u Ã½: Äáº£m báº£o báº¡n cÃ³ quyá»n sá»­ dá»¥ng API GPT-OSS-120B vÃ  tuÃ¢n thá»§ cÃ¡c Ä‘iá»u khoáº£n sá»­ dá»¥ng cá»§a nhÃ  cung cáº¥p.*